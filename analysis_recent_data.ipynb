{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence_transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence_transformers) (2.4.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence_transformers) (0.25.0)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (73.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/python/3.12.1/lib/python3.12/site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/codespace/.local/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from xgboost) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from xgboost) (2.23.4)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from xgboost) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/codespace/.local/lib/python3.12/site-packages (from lightgbm) (2.1.0)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from lightgbm) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspaces/misinformation/df_2829_manualCoding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>author.id</th>\n",
       "      <th>author.userName</th>\n",
       "      <th>author.followers</th>\n",
       "      <th>author.following</th>\n",
       "      <th>author.createdAt</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>date</th>\n",
       "      <th>createdDate</th>\n",
       "      <th>misinformation</th>\n",
       "      <th>stance</th>\n",
       "      <th>isUSRelated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T2</td>\n",
       "      <td>1224387</td>\n",
       "      <td>1479667332293160967</td>\n",
       "      <td>Fox News: Harris comms chief apologizes after ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sat Jan 08 04:12:37 +0000 2022</td>\n",
       "      <td>303850691</td>\n",
       "      <td>01splcheck</td>\n",
       "      <td>7806</td>\n",
       "      <td>6911</td>\n",
       "      <td>Mon May 23 14:37:18 +0000 2011</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-01-07 20:12:37</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>T</td>\n",
       "      <td>U</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T4</td>\n",
       "      <td>332659</td>\n",
       "      <td>1778181695575105704</td>\n",
       "      <td>@Cartel_Cal @RepRobertGarcia It seems he's in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wed Apr 10 22:02:07 +0000 2024</td>\n",
       "      <td>303850691</td>\n",
       "      <td>01splcheck</td>\n",
       "      <td>7789</td>\n",
       "      <td>6899</td>\n",
       "      <td>Mon May 23 14:37:18 +0000 2011</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-10 15:02:07</td>\n",
       "      <td>2024-04-10</td>\n",
       "      <td>CF</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>1567887</td>\n",
       "      <td>1656440842830376960</td>\n",
       "      <td>@NatlyDenise_ Ready to bring illegal immigrant...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wed May 10 23:27:26 +0000 2023</td>\n",
       "      <td>303850691</td>\n",
       "      <td>01splcheck</td>\n",
       "      <td>7814</td>\n",
       "      <td>6921</td>\n",
       "      <td>Mon May 23 14:37:18 +0000 2011</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-05-10 16:27:26</td>\n",
       "      <td>2023-05-10</td>\n",
       "      <td>CF</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T2</td>\n",
       "      <td>1303080</td>\n",
       "      <td>1477882502807052290</td>\n",
       "      <td>@cassstastrophe Well, you can take comfort tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jan 03 06:00:21 +0000 2022</td>\n",
       "      <td>303850691</td>\n",
       "      <td>01splcheck</td>\n",
       "      <td>7806</td>\n",
       "      <td>6911</td>\n",
       "      <td>Mon May 23 14:37:18 +0000 2011</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-01-02 22:00:21</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>1578404</td>\n",
       "      <td>1656441829464870912</td>\n",
       "      <td>I see a press conference with Arizona official...</td>\n",
       "      <td>1</td>\n",
       "      <td>Wed May 10 23:31:22 +0000 2023</td>\n",
       "      <td>303850691</td>\n",
       "      <td>01splcheck</td>\n",
       "      <td>7814</td>\n",
       "      <td>6921</td>\n",
       "      <td>Mon May 23 14:37:18 +0000 2011</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-05-10 16:31:22</td>\n",
       "      <td>2023-05-10</td>\n",
       "      <td>UV</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  origin  Unnamed: 0                   id  \\\n",
       "0     T2     1224387  1479667332293160967   \n",
       "1     T4      332659  1778181695575105704   \n",
       "2     T3     1567887  1656440842830376960   \n",
       "3     T2     1303080  1477882502807052290   \n",
       "4     T3     1578404  1656441829464870912   \n",
       "\n",
       "                                                text  retweetCount  \\\n",
       "0  Fox News: Harris comms chief apologizes after ...             0   \n",
       "1  @Cartel_Cal @RepRobertGarcia It seems he's in ...             0   \n",
       "2  @NatlyDenise_ Ready to bring illegal immigrant...             0   \n",
       "3  @cassstastrophe Well, you can take comfort tha...             0   \n",
       "4  I see a press conference with Arizona official...             1   \n",
       "\n",
       "                        CreatedAt  author.id author.userName  \\\n",
       "0  Sat Jan 08 04:12:37 +0000 2022  303850691      01splcheck   \n",
       "1  Wed Apr 10 22:02:07 +0000 2024  303850691      01splcheck   \n",
       "2  Wed May 10 23:27:26 +0000 2023  303850691      01splcheck   \n",
       "3  Mon Jan 03 06:00:21 +0000 2022  303850691      01splcheck   \n",
       "4  Wed May 10 23:31:22 +0000 2023  303850691      01splcheck   \n",
       "\n",
       "   author.followers  author.following                author.createdAt  \\\n",
       "0              7806              6911  Mon May 23 14:37:18 +0000 2011   \n",
       "1              7789              6899  Mon May 23 14:37:18 +0000 2011   \n",
       "2              7814              6921  Mon May 23 14:37:18 +0000 2011   \n",
       "3              7806              6911  Mon May 23 14:37:18 +0000 2011   \n",
       "4              7814              6921  Mon May 23 14:37:18 +0000 2011   \n",
       "\n",
       "   isRetweet                 date createdDate misinformation stance  \\\n",
       "0      False  2022-01-07 20:12:37  2022-01-07              T      U   \n",
       "1      False  2024-04-10 15:02:07  2024-04-10             CF      A   \n",
       "2      False  2023-05-10 16:27:26  2023-05-10             CF      A   \n",
       "3      False  2022-01-02 22:00:21  2022-01-02              E      A   \n",
       "4      False  2023-05-10 16:31:22  2023-05-10             UV      A   \n",
       "\n",
       "  isUSRelated  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "misinformation\n",
       "T                              1422\n",
       "E                               677\n",
       "CF                              242\n",
       "F                               207\n",
       "UV                              113\n",
       "unsupported/low credibility     102\n",
       "NR                               66\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['misinformation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['misinformation'] != 'NR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39088/1615463619.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['opinion'] = filtered_df['misinformation'].apply(lambda x: 1 if x == 'CF' else 0)\n"
     ]
    }
   ],
   "source": [
    "filtered_df['opinion'] = filtered_df['misinformation'].apply(lambda x: 1 if x == 'CF' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n",
      "Error occurred: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39088/2479277127.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['claimbuster_score'] = filtered_df['text'].apply(get_claimbuster_score)\n"
     ]
    }
   ],
   "source": [
    "api_key = \"1d4a3eb3686441c0924adf0165146cb4\"\n",
    "\n",
    "def get_claimbuster_score(text):\n",
    "    try:\n",
    "        # Define the endpoint with the claim as part of it\n",
    "        api_endpoint = f\"https://idir.uta.edu/claimbuster/api/v2/score/text/{text}\"\n",
    "        request_headers = {\"x-api-key\": api_key}\n",
    "\n",
    "        response = requests.get(url=api_endpoint, headers=request_headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['results'][0]['score']  \n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            return None  \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None \n",
    "\n",
    "filtered_df['claimbuster_score'] = filtered_df['text'].apply(get_claimbuster_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39088/2820307767.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['processed_text'] = filtered_df['text'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "filtered_df['processed_text'] = filtered_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_ = filtered_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_df_[['processed_text', 'claimbuster_score']]\n",
    "y = filtered_df_['opinion']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opinion\n",
       "0    2440\n",
       "1     236\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 67/67 [00:33<00:00,  2.01it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:09<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "\n",
    "\n",
    "def embed_text(text_series):\n",
    "    return np.array(bert_model.encode(text_series.tolist(), batch_size=32, show_progress_bar=True))\n",
    "\n",
    "X_train_embeddings = np.array(bert_model.encode(X_train['processed_text'].tolist(), batch_size=32, show_progress_bar=True))\n",
    "X_test_embeddings = np.array(bert_model.encode(X_test['processed_text'].tolist(), batch_size=32, show_progress_bar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_score_scaled = scaler.fit_transform(X_train[['claimbuster_score']])\n",
    "X_test_score_scaled = scaler.transform(X_test[['claimbuster_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = np.hstack((X_train_embeddings, X_train_score_scaled))\n",
    "X_test_combined = np.hstack((X_test_embeddings, X_test_score_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86       495\n",
      "           1       0.19      0.63      0.29        41\n",
      "\n",
      "    accuracy                           0.76       536\n",
      "   macro avg       0.58      0.70      0.58       536\n",
      "weighted avg       0.90      0.76      0.82       536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_combined, y_train)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred = model.predict(X_test_combined)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89       495\n",
      "           1       0.20      0.51      0.28        41\n",
      "\n",
      "    accuracy                           0.80       536\n",
      "   macro avg       0.57      0.67      0.58       536\n",
      "weighted avg       0.90      0.80      0.84       536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC(class_weight='balanced', probability=True)\n",
    "model.fit(X_train_combined, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_combined)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 195, number of negative: 1945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98175\n",
      "[LightGBM] [Info] Number of data points in the train set: 2140, number of used features: 385\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       495\n",
      "           1       0.56      0.12      0.20        41\n",
      "\n",
      "    accuracy                           0.93       536\n",
      "   macro avg       0.74      0.56      0.58       536\n",
      "weighted avg       0.90      0.93      0.90       536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train_combined, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_combined)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39088/1337875297.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_opinion_df['non_opinion_label'] = non_opinion_df['misinformation'].map({\n"
     ]
    }
   ],
   "source": [
    "non_opinion_df = filtered_df_[(filtered_df_['opinion'] == 0) & (filtered_df_['misinformation'] != 'UV')]\n",
    "\n",
    "# Map non-opinion classes to True and False\n",
    "non_opinion_df['non_opinion_label'] = non_opinion_df['misinformation'].map({\n",
    "    'T': 'True',\n",
    "    'F': 'False',\n",
    "    'E': 'False',\n",
    "    'unsupported/low credibility': 'False'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39088/1963711333.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_opinion_df['non_opinion_encoded'] = le_non_opinion.fit_transform(non_opinion_df['non_opinion_label'])\n"
     ]
    }
   ],
   "source": [
    "le_non_opinion = LabelEncoder()\n",
    "non_opinion_df['non_opinion_encoded'] = le_non_opinion.fit_transform(non_opinion_df['non_opinion_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_non_opinion = non_opinion_df[['processed_text', 'claimbuster_score']] \n",
    "y_non_opinion = non_opinion_df['non_opinion_encoded']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_non_opinion, y_non_opinion, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 59/59 [00:30<00:00,  1.93it/s]\n",
      "Batches: 100%|██████████| 15/15 [00:08<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "X_train_embeddings = bert_model.encode(X_train['processed_text'].tolist(), batch_size=32, show_progress_bar=True)\n",
    "X_test_embeddings = bert_model.encode(X_test['processed_text'].tolist(), batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train['claimbuster_score_scaled'] = scaler.fit_transform(X_train[['claimbuster_score']])\n",
    "X_test['claimbuster_score_scaled'] = scaler.transform(X_test[['claimbuster_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = np.hstack((X_train_embeddings, X_train[['claimbuster_score']].values))\n",
    "X_test_combined = np.hstack((X_test_embeddings, X_test[['claimbuster_score']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.73      0.69       184\n",
      "        True       0.81      0.74      0.78       282\n",
      "\n",
      "    accuracy                           0.74       466\n",
      "   macro avg       0.73      0.74      0.73       466\n",
      "weighted avg       0.75      0.74      0.74       466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lgb_clf = LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "svc_clf = SVC(class_weight='balanced', probability=True)\n",
    "svc_clf.fit(X_train_combined, y_train)\n",
    "\n",
    "y_pred = svc_clf.predict(X_test_combined)\n",
    "print(classification_report(y_test, y_pred, target_names=le_non_opinion.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "#     'XGBoost': XGBClassifier(scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1])),\n",
    "#     'LightGBM': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "#     'SVM': SVC(class_weight='balanced', probability=True)\n",
    "# }\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"Training {model_name}...\")\n",
    "#     model.fit(X_train_combined, y_train)\n",
    "#     y_pred = model.predict(X_test_combined)\n",
    "#     print(f\"Results for {model_name}:\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39088/704255181.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_['misinformation_encoded'] = le.fit_transform(filtered_df_['misinformation'])\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "filtered_df_['misinformation_encoded'] = le.fit_transform(filtered_df_['misinformation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_df_[['processed_text', 'claimbuster_score']] \n",
    "y = filtered_df_['misinformation_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 67/67 [00:33<00:00,  2.02it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:09<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "X_train_embeddings = bert_model.encode(X_train['processed_text'].tolist(), batch_size=32, show_progress_bar=True)\n",
    "X_test_embeddings = bert_model.encode(X_test['processed_text'].tolist(), batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train['claimbuster_score_scaled'] = scaler.fit_transform(X_train[['claimbuster_score']])\n",
    "X_test['claimbuster_score_scaled'] = scaler.transform(X_test[['claimbuster_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = np.hstack((X_train_embeddings, X_train[['claimbuster_score']].values))\n",
    "X_test_combined = np.hstack((X_test_embeddings, X_test[['claimbuster_score']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98175\n",
      "[LightGBM] [Info] Number of data points in the train set: 2140, number of used features: 385\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                         CF       0.45      0.22      0.30        41\n",
      "                          E       0.45      0.48      0.46       126\n",
      "                          F       1.00      0.03      0.06        34\n",
      "                          T       0.68      0.91      0.78       286\n",
      "                         UV       1.00      0.07      0.12        30\n",
      "unsupported/low credibility       0.00      0.00      0.00        19\n",
      "\n",
      "                   accuracy                           0.62       536\n",
      "                  macro avg       0.60      0.28      0.29       536\n",
      "               weighted avg       0.62      0.62      0.56       536\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "lgb_clf = LGBMClassifier(class_weight='balanced', random_state=42, num_class=len(le.classes_))\n",
    "lgb_clf.fit(X_train_combined, y_train)\n",
    "\n",
    "y_pred = lgb_clf.predict(X_test_combined)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
